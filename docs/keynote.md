# Introduction to Diffusion Models —— GIS Lab 2024 Short-term Course

> [!NOTE]
> Keynote for "Introduction to Diffusion Models —— GIS Lab 2024 Short-term Course" is available on BaiduNetdisk [here](https://pan.baidu.com/s/1NAZi_NWV3lNLi1rNXhJxhA?pwd=0702).

> Presentor: Sakura (Chen Zhenyuan) M.Eng.
>
> Department: School of Earth Science, Zhejiang University
>
> Date: July 2, 2024
>
> Contact me: [bili_sakura@zju.edu.cn](mailto:bili_sakura@zju.edu.cn)

## Copyright Statement

© Sakura, 2024. All rights reserved. This document is protected by copyright law. No part of this publication may be reproduced, distributed, or transmitted in any form or by any means, including photocopying, recording, or other electronic or mechanical methods, without the prior written permission of the author, except in the case of brief quotations embodied in critical reviews and certain other noncommercial uses permitted by copyright law. For permission requests, please contact the author at [bili_sakura@zju.edu.cn](mailto:bili_sakura@zju.edu.cn).

![](../assets/sakura.png)

## Outline

- [Background](#background)

  - Prosperity of Generative Artificial Intelligence (Gen AI)

  - Progress of Deep Generative Models
- [Introduction](#introduction)
  - Generation from Scratch
  - Auto-regression
  - Generalized Auto-regression
  - Denoising Diffusion
- [Techniques of Diffusion Models](#techniques-of-diffusion-models)
  - Evolution of Architectures
  - Conditional Generation
  - Robustness
- [**Practice on Diffusion Model**](#practice-on-diffusion-model)
  - Diffusion Model in Remote Sensing (RS)
  - Research Background
  - Experiments

## Background

### Prosperity of Gen AI: Midjourney

<div align="center"><img src="../assets/midjourney_cases.png"><p>Awesome pictures generated by Midjourney V5.*
</p></div>

> <https://legacy.midjourney.com/showcase/>

### Prosperity of Gen AI: DALL·E 3

<div align="center"><img src="../assets/DALLE-3_case1_full.png"><img src="../assets/DALLE-3_case2_full.png"><p>DALL·E 3 is capable of understanding complex instructions.*</p></div>

> <https://openai.com/blog/dall-e-3-is-now-available-in-chatgpt-plus-and-enterprise>

### Prosperity of Gen AI: Gen-2

<div align="center"><img src="../assets/WebGEN2R_T2V.gif"><p>Text to Video by Gen-2. *</p><p> (The late afternoon sun peeking through the window of a New York City loft.)</p></div>

> <https://research.runwayml.com/gen2>

### Prosperity of Gen AI: Pika

<div align="center"><img src="../assets/cinematic,_aerial_drone_flythough_over_seashore_of_the_vast_mountain_cliffs_of_Scotland,_strong_bree_seed6089107589751151_upscaled.gif"><p>Text to Video. *</p><p>(cinematic, aerial drone flythough over seashore of the vast mountain cliffs of Scotland, strong breeze blowing hair and strong ocean waves, dolly in)</p></div>

> <https://pika.art/>

### Progress of Deep Generative Models

<div align="center">
    <img src="../assets/timeline_of_gen.png">
    <p>Progress of deep generative models <a href="https://thudzj.github.io/">[1]</a>.</p>
</div>

## Introduction

### Generation from Scratch

<div align="center">
<img src="../assets/vgg.png">
<img src="../assets/vgg_visualization.png">
<p>Above: Architecture of VGG <a href="https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108">[2]</a>. Bottom: The illustration of external and internal behavior of a CNN <a href="https://www.semanticscholar.org/paper/Visualizing-and-Comparing-AlexNet-and-VGG-using-Yu-Yang/dae981902b1f6d869ef2d047612b90cdbe43fd1e">[3]</a>. (VGG team won ILSVRC 2014 <a href="http://arxiv.org/abs/1409.0575">[4]</a> with 1st in localization Task and 2nd in classification task.</p>
</div>

<div align="center">
<div style="display: flex; justify-content: center;">
    <img src="../assets/video_clip0.gif">
    <img src="../assets/video_clip1.gif">
</div>
  <p>Why Naïve Generation Doesn't Work. *</p>
<div style="display: flex; justify-content: center;">
    <img src="../assets/average_for_label.jpg" width="50%">
    <img src="../assets/video_clip2.gif">
</div>
<p>Left: Average makes sense in label prediction.*</p>
<p>Right: Average does not make sense in pixel level prediction.*</p>
</div>

### Auto-regression

<div align="center">
<div style="display: flex; justify-content: center;">
    <img src="../assets/video_clip3.gif">
    <img src="../assets/video_clip4.gif">
</div>
<p>Left: A neural network (purple) predicts the last pixel.*</p>
<p>Right: A neural network (pink) predicts the second last pixel.*</p>
</div>
<div align="center">
    <img src="../assets/video_clip5.gif" width="150%">
<p>We succeed in generating an image from nothing by predicting the next pixel.*</p>
</div>
<div align="center">
<div style="display: flex; justify-content: center;">
    <img src="../assets/pixel_probability.jpg" width="50%">
    <img src="../assets/video_clip6.gif">
</div>
<p>Left: A determined neural network give the identical result
given the same first pixel value. *</p>
<p>Right: We can randomly sample pixel value in each step to generate different images from identical initial pixel. *</p>
</div>

<div align="center">
    <img src="../assets/video_clip7.gif">
<p> Auto regressor for image generation. *</p>
    <img src="../assets/gpt-2-autoregression-2.gif">
<p>Auto regressor for text generation. *</p>
</div>

> <https://www.youtube.com/watch?v=zc5NTeJbk-k>

### Generalized Auto-regression

<div align="center">
<img src="../assets/video_clip8.gif">
<p>Predict a whole block instead of single pixel.*</p>
<img src="../assets/average_plausible_results.png">
<p>Average in pixel value leads to non-sense.*</p>
<img src="../assets/independent_pixel_prediction.png">
<p>We can feel free to predict any pixel seperately, if pixel value is statistically independent.*</p>
    <img src="../assets/similarity_in_image.png">
    <p>It is noted that near pixels are the most strongly related, because they are usaually part of the same object.*</p>
    <img src="../assets/remove_seperately_cases.png">
    <p>Remove separate pixels from the image and train a network to predict them (as these pixels are far from each other, we regard them as independent).*
</p>
</div>

<div align="center">
    <img src="../assets/video_clip9.gif">
<p> Remove indepedent pixels graudually (the reversed process is to predict the image).*</p>
</div>

<div align="center">
    <img src="../assets/regressor_recover_process_full.png">
<p> Auto regressor generate images in two steps (add masks & remove masks).*</p>
</div>

<div align="center">
    <img src="../assets/PixelCNN_overview.png">
<p> Illustration of PixelCNN <a href="https://proceedings.mlr.press/v48/oord16.html">[5]</a>.(Left: To generate pixel xi one conditions on all the previously generated pixels left and above of xi. Center: To generate a pixel in the multi-scale case we can also condition on the subsampled image pixels. Right: Diagram of the connectivity inside a masked convolution. In the first layer, each of the RGB channels is connected to previous channels and to the context, but is not connected to itself. In subsequent layers, the channels are also connected to themselves.)</p>
</div>

> <https://www.youtube.com/watch?v=zc5NTeJbk-k>

### Denoising Diffusion

<div align="center">
    <img src="../assets/video_clip10.gif">
<p> Remove information by add noise rather than completely remove pixels.*</p>
</div>

> <https://www.youtube.com/watch?v=zc5NTeJbk-k>

<div align="center">
    <img src="../assets/add_noice_denoise2.png">
    An illustration of foreward/backward process for denoising diffusion models <a href="https://openreview.net/forum?id=PxTIG12RRHS&utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter">[6]</a>.
    <img src="../assets/DDPM_algorithm.png">
Algorithm of denoising diffusion probabilistic models (DDPM) <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">[15]</a>.
</div>

## Techniques of Diffusion Models

### Evolution of Architectures

<div align="center"><img src="../assets/evolution_of_diffusion.png">
    <p>Left: Architecture of a CNN-based diffusion model <a href="https://proceedings.mlr.press/v37/sohl-dickstein15.html">[7]</a> with training dataset (a) and generation samples (b).</p>
    <p>Center: Architecture of U-ViT <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bao_All_Are_Worth_Words_A_ViT_Backbone_for_Diffusion_Models_CVPR_2023_paper.html">[8]</a>.
</p>
    <p>Right: Fundemantal archicture of U-ViT. above: ViT <a href="https://openreview.net/forum?id=YicbFdNTTy">[9]</a>; below: U-Net <a href="http://link.springer.com/10.1007/978-3-319-24574-4_28">[10]</a>. </p>
</div>

### Conditional Generation

<div align="center"><img src="../assets/video_clip11.gif"><p>
    Conditional image generation with sketch.*
    </p></div>

> [https](https://www.youtube.com/watch?v=zc5NTeJbk-k)[://](https://www.youtube.com/watch?v=zc5NTeJbk-k)[www.youtube.com/watch?v=zc5NTeJbk-k](https://www.youtube.com/watch?v=zc5NTeJbk-k)
>
> <https://www.canva.com/>

### Robustness

<div align="center"><img src="../assets/attacks_on_stable_diffusion.png"><p> An illustration of human attack against Stable Diffusion*. Adversarially modified content is highlighted in red <a href="http://arxiv.org/abs/2306.13103">[11]</a>.</p></div>

> <https://stability.ai/news/stable-diffusion-3>

## Practice on Diffusion Model

### Diffusion Model in Remote Sensing

<div align="center"><img src="../assets/MetaEarth.png"><p> MetaEarth <a href="http://arxiv.org/abs/2405.13570">[12]</a>: A generative foundation model for global-scale remote sensing image generation. Remote sensing image generation samples. </p><img src="../assets/MetaEarth_architecture.png"><p>Overview of MetaEarth architecture <a href="http://arxiv.org/abs/2405.13570">[12]</a>.</p>
</div>

<div align="center"><img src="../assets/DiffusionSAT_overview.png"><p> Conditioning on freely available metadata and using large, publicly available satellite imagery datasets shows DiffusionSat <a href="http://arxiv.org/abs/2312.03606">[13]</a> is a powerful generative foundation model for remote sensing data. Left: Overview of architecture. </p><img src="../assets/DiffusionSAT_pre_post_pair_cases.png"><p>Onpainting results <a href="http://arxiv.org/abs/2312.03606">[13]</a>.</p>
</div>

### Research Background

<div align="center">
    <div style="display: flex; justify-content: center;">
        <img src="../assets/xbd_description1.png" width="80%">
        <img src="../assets/xbd_example.png" width="35%">
    </div>
    <p> Left: Disaster types and disasters represented in xBD <a href="http://arxiv.org/abs/1911.09296">[14]</a> around the world.
Right: Building polygons (shown in green) on imagery from Hurricane Michael (2018).
</p>
</div>

**Abstract**: [Background] Object Detection of Damaged Buildings in disastrous event is important for aiding and reconstruction. [Problem] Current approaches for building damage assessment include CNN-based models and Transformer-based models. However, these pre-trained model is still lack of general capability which fails to timeliness of detection in terms of disastrous events. [Method] In this work, we propose a generative model *ModelName* to manufacture potential post-disaster images from vulnerable regions at global scale, be it Global Building Damage (GBD) dataset. [Result] We find that after further training SOTA models on GBD, the performance of models show great improvements.

<div align="center">
   <img src="../assets/GBD_Overview.png">
    <p> Overview of our GBD framework.</p>
</div>

### Experiments

Course Objectives:

\1. Learn to use Github for team collaboration

\2. Acquire the ability to configure a deep learning environment on a server

\3. Gain the capability to deploy and run deep learning models (diffusion models) on a server

\4. Understand the basic principles of diffusion models

\5. Become familiar with common remote sensing datasets/data sources

\6. Use the DiffusionSAT model to generate disaster remote sensing images

\7. Attempt to build or run condition-constrained diffusion models to generate pre- and post-disaster remote sensing image pairs

课程目标：

\1. 学会使用Github进行团队协作

\2. 具备配置服务器深度学习环境的能力

\3. 具备在服务器上部署并运行深度学习模型（扩散模型）的能力

\4. 了解扩散模型的基本原理

\5. 熟悉常用遥感数据集/数据源

\6. 使用DiffusionSAT模型生成灾害遥感影像

\7. 尝试构建或运行具有条件约束的扩散模型以生成灾害前后遥感影像对

## References

[1] Z. Deng, "扩散模型: 方法与应用" [Advanced Neural Networks, Spring, 2024], Qing Yuan Research Institute, Shanghai Jiao Tong University, 2024.

[2] K. Simonyan and A. Zisserman, ‘Very Deep Convolutional Networks for Large-Scale Image Recognition’, in International Conference on Learning Representations, 2014.

[3] W. Yu, K. Yang, T. Xiao, H. Yao, and Y. Rui, ‘Visualizing and Comparing AlexNet and VGG using Deconvolutional Layers’, 2016.

[4] O. Russakovsky et al., ‘ImageNet Large Scale Visual Recognition Challenge’. arXiv, Jan. 29, 2015.

[5] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu, ‘Pixel Recurrent Neural Networks’, in Proceedings of The 33rd International Conference on Machine Learning, PMLR, Jun. 2016, pp. 1747–1756.

[6] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, ‘Score-Based Generative Modeling through Stochastic Differential Equations’, presented at the International Conference on Learning Representations, Oct. 2020.

[7] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, ‘Deep Unsupervised Learning using Nonequilibrium Thermodynamics’, in Proceedings of the 32nd International Conference on Machine Learning, PMLR, Jun. 2015, pp. 2256–2265.

[8] F. Bao et al., ‘All Are Worth Words: A ViT Backbone for Diffusion Models’, presented at the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 22669–22679.

[9] A. Dosovitskiy et al., ‘An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale’, in International Conference on Learning Representations, Oct. 2020.

[10] O. Ronneberger, P. Fischer, and T. Brox, ‘U-Net: Convolutional Networks for Biomedical Image Segmentation’, in International Conference on Medical Image Computing and Computer-Assisted Intervention, N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, Eds., Cham: Springer International Publishing, 2015, pp. 234–241.

[11] H. Gao, H. Zhang, Y. Dong, and Z. Deng, ‘Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks’. arXiv, Jun. 15, 2023.

[12] Z. Yu, C. Liu, L. Liu, Z. Shi, and Z. Zou, ‘MetaEarth: A Generative Foundation Model for Global-Scale Remote Sensing Image Generation’. arXiv. May 28, 2024.

[13] S. Khanna et al., ‘DiffusionSat: A Generative Foundation Model for Satellite Imagery’, in International Conference on Learning Representations (ICLR 2024), Dec. 2023.

[14] R. Gupta et al., ‘xBD: A Dataset for Assessing Building Damage from Satellite Imagery’. arXiv, Nov. 21, 2019.

[15] J. Ho, A. Jain, and P. Abbeel, ‘Denoising Diffusion Probabilistic Models’, in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2020, pp. 6840–6851.
